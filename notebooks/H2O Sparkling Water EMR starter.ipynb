{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H2O Sparkiling Water EMR Starter (Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebooks assumes that following environment variables are set:\n",
    "\n",
    "```\n",
    "PATH=$PATH:$HOME/.local/bin:$HOME/bin\n",
    "export PATH\n",
    "export PATH=/mnt/opt/anaconda520:/mnt/opt/anaconda520/bin:$PATH\n",
    "export PYSPARK_PYTHON=/mnt/opt/anaconda520/bin/python\n",
    "export SPARK_HOME=/usr/lib/spark\n",
    "export HADOOP_CONF_DIR=/etc/hadoop/conf\n",
    "export MASTER=\"yarn\"\n",
    "```\n",
    "\n",
    "It also assumes that Pysparkling Python module can be found here :\n",
    "\n",
    "`/mnt/opt/sparkling-water-2.4.11/py/build/dist/h2o_pysparkling_2.4-2.4.11.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pyspark version: 2.4.0\n",
      "Pyspark module location: /mnt/opt/anaconda520/lib/python3.6/site-packages/pyspark/__init__.py\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder.appName(\"Example_Notebook\") \\\n",
    "        .config(\"spark.submit.deployMode\",\"client\") \\\n",
    "        .config(\"spark.dynamicAllocation.enabled\",False) \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "\n",
    "import pyspark\n",
    "print(\"Pyspark version:\", pyspark.__version__)\n",
    "print(\"Pyspark module location:\", pyspark.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-46-145.us-east-2.compute.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Example_Notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f93d112a438>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional - Get parameters for config_pyspark by running and replace some of them\n",
    "\n",
    "This might also fix problem with environment variables and not being able to connect to the YARN cluster.\n",
    "\n",
    "Run:\n",
    "`pyspark` and then \n",
    "`sc.getConf().getAll()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "config_pyspark =[(u'spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES', u'http://ip-172-31-36-110.us-east-2.compute.internal:20888/proxy/application_1558398699025_0001'), (u'spark.eventLog.enabled', u'true'), (u'spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version.emr_internal_use_only.EmrFileSystem', u'2'), (u'spark.yarn.executor.memoryOverheadFactor', u'0.1875'), (u'spark.driver.extraLibraryPath', u'/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native'), (u'spark.sql.parquet.output.committer.class', u'com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter'), (u'spark.app.id', u'application_1558398699025_0001'), (u'spark.driver.appUIAddress', u'http://ip-172-31-36-110.us-east-2.compute.internal:4040'), (u'spark.blacklist.decommissioning.timeout', u'1h'), (u'spark.yarn.historyServer.address', u'ip-172-31-36-110.us-east-2.compute.internal:18080'), (u'spark.driver.host', u'ip-172-31-36-110.us-east-2.compute.internal'), (u'spark.yarn.appMasterEnv.SPARK_PUBLIC_DNS', u'$(hostname -f)'), (u'spark.executor.cores', u'4'), (u'spark.sql.emr.internal.extensions', u'com.amazonaws.emr.spark.EmrSparkSessionExtensions'), (u'spark.executor.extraJavaOptions', u\"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled -XX:OnOutOfMemoryError='kill -9 %p'\"), (u'spark.eventLog.dir', u'hdfs:///var/log/spark/apps'), (u'spark.sql.hive.metastore.sharedPrefixes', u'com.amazonaws.services.dynamodbv2'), (u'spark.sql.warehouse.dir', u'hdfs:///user/spark/warehouse'), (u'spark.serializer.objectStreamReset', u'100'), (u'spark.executorEnv.PYTHONPATH', u'/usr/lib/spark/python/lib/py4j-0.10.7-src.zip:/usr/lib/spark/python/:<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.7-src.zip'), (u'spark.submit.deployMode', u'client'), (u'spark.ui.proxyBase', u'/proxy/application_1558398699025_0001'), (u'spark.history.fs.logDirectory', u'hdfs:///var/log/spark/apps'), (u'spark.ui.filters', u'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'), (u'spark.sql.parquet.fs.optimized.committer.optimization-enabled', u'true'), (u'spark.driver.extraClassPath', u'/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar'), (u'spark.hadoop.mapreduce.fileoutputcommitter.cleanup-failures.ignored.emr_internal_use_only.EmrFileSystem', u'true'), (u'spark.history.ui.port', u'18080'), (u'spark.shuffle.service.enabled', u'true'), (u'spark.executor.memory', u'4269M'), (u'spark.hadoop.yarn.timeline-service.enabled', u'false'), (u'spark.driver.port', u'32873'), (u'spark.resourceManager.cleanupExpiredHost', u'true'), (u'spark.executor.id', u'driver'), (u'spark.executor.extraClassPath', u'/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar'), (u'spark.yarn.dist.files', u'file:/etc/spark/conf/hive-site.xml'), (u'spark.app.name', u'PySparkShell'), (u'spark.files.fetchFailure.unRegisterOutputOnHost', u'true'), (u'spark.driver.extraJavaOptions', u\"-XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled -XX:OnOutOfMemoryError='kill -9 %p'\"), (u'spark.master', u'yarn'), (u'spark.decommissioning.timeout.threshold', u'20'), (u'spark.sql.catalogImplementation', u'hive'), (u'spark.stage.attempt.ignoreOnDecommissionFetchFailure', u'true'), (u'spark.rdd.compress', u'True'), (u'spark.executor.extraLibraryPath', u'/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native'), (u'spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS', u'ip-172-31-36-110.us-east-2.compute.internal'), (u'spark.yarn.isPython', u'true'), (u'spark.dynamicAllocation.enabled', u'true'), (u'spark.ui.showConsoleProgress', u'true'), (u'spark.blacklist.decommissioning.enabled', u'true')]\n",
    "remove_params =[]\n",
    "for configs in config_pyspark:\n",
    "    #print(configs)\n",
    "    if configs[0] in ['spark.app.name','spark.app.id','spark.history.ui.port','spark.executor.memory'\n",
    "                      ,'spark.dynamicAllocation.enabled']:\n",
    "        print('Removing:', configs)\n",
    "        remove_params.append(configs)\n",
    "for param in remove_params:\n",
    "    config_pyspark.remove(param)\n",
    "\n",
    "#config_p.remove(('spark.app.name', 'PySparkShell'))\n",
    "config_pyspark.append(('spark.executor.memory', '2176M')) \n",
    "config_pyspark.append(('spark.dynamicAllocation.enabled',False))\n",
    "print('Number of parameters', len(config_pyspark))\n",
    "\n",
    "from pyspark.conf import SparkConf\n",
    "config_params = spark.sparkContext._conf.setAll(config_pyspark)\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Example_Notebook\") \\\n",
    "        .config(conf=config_params) \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "spark\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/mnt/opt/sparkling-water-2.4.11/py/build/dist/h2o_pysparkling_2.4-2.4.11.zip')\n",
    "import pysparkling\n",
    "from pysparkling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to H2O server at http://ip-172-31-46-145.us-east-2.compute.internal:54321 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>15 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Etc/UTC</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.24.0.3</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>15 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>sparkling-water-jupyterlab_application_1558494441471_0003</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>2</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>8.31 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://ip-172-31-46-145.us-east-2.compute.internal:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>XGBoost, Algos, Amazon S3, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.5 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ---------------------------------------------------------\n",
       "H2O cluster uptime:         15 secs\n",
       "H2O cluster timezone:       Etc/UTC\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.24.0.3\n",
       "H2O cluster version age:    15 days\n",
       "H2O cluster name:           sparkling-water-jupyterlab_application_1558494441471_0003\n",
       "H2O cluster total nodes:    2\n",
       "H2O cluster free memory:    8.31 Gb\n",
       "H2O cluster total cores:    4\n",
       "H2O cluster allowed cores:  8\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://ip-172-31-46-145.us-east-2.compute.internal:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         XGBoost, Algos, Amazon S3, AutoML, Core V3, Core V4\n",
       "Python version:             3.6.5 final\n",
       "--------------------------  ---------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sparkling Water Context:\n",
      " * H2O name: sparkling-water-jupyterlab_application_1558494441471_0003\n",
      " * cluster size: 2\n",
      " * list of used nodes:\n",
      "  (executorId, host, port)\n",
      "  ------------------------\n",
      "  (2,ip-172-31-35-227.us-east-2.compute.internal,54321)\n",
      "  (1,ip-172-31-36-140.us-east-2.compute.internal,54321)\n",
      "  ------------------------\n",
      "\n",
      "  Open H2O Flow in browser: http://ip-172-31-46-145.us-east-2.compute.internal:54321 (CMD + click in Mac OSX)\n",
      "\n",
      "    \n",
      " * Yarn App ID of Spark application: application_1558494441471_0003\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import h2o\n",
    "hc = H2OContext.getOrCreate(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
